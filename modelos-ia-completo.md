# Modelos de IA: Preprocesamiento, Hiperparámetros y Métricas

## 1. Modelos para Datos Tabulares

| Modelo | Preprocesamiento de Datos | Hiperparámetros Clave | Métricas de Evaluación |
|--------|---------------------------|------------------------|------------------------|
| **Regresión Lineal/Logística** | • Manejo de valores nulos (imputación)<br>• Normalización/Estandarización<br>• Codificación one-hot de variables categóricas<br>• Detección y manejo de outliers<br>• Transformación de variables sesgadas<br>• Reducción de dimensionalidad (PCA, LDA) | • Regularización (L1/L2)<br>• Coeficiente de regularización (C o alpha)<br>• Tolerancia de convergencia<br>• Solver (liblinear, saga, etc.)<br>• Umbral de clasificación (para regresión logística)<br>• Tipo de penalización | • Regresión: MAE, MSE, RMSE, R²<br>• Clasificación: Precisión, Recall, F1-score, AUC-ROC, Log loss |
| **Árboles de Decisión** | • Manejo de valores nulos (imputación)<br>• Codificación de variables categóricas<br>• No requiere normalización<br>• Balanceo de clases para datos desbalanceados | • Profundidad máxima<br>• Mínimo de muestras para división<br>• Mínimo de muestras en hoja<br>• Criterio de división (gini, entropy)<br>• Estrategia de poda<br>• Pesos de clase | • Precisión, Recall, F1-score<br>• Importancia de características<br>• Área bajo la curva ROC<br>• Matriz de confusión |
| **Random Forest** | • Manejo de valores nulos (imputación)<br>• Codificación de variables categóricas<br>• No requiere normalización<br>• Balanceo de clases para datos desbalanceados | • Número de árboles<br>• Profundidad máxima<br>• Mínimo de muestras para división<br>• Mínimo de muestras en hoja<br>• Bootstrap<br>• Máximo de características<br>• Criterio de división | • Precisión, Recall, F1-score<br>• Importancia de características<br>• Out-of-bag error<br>• AUC-ROC |
| **Gradient Boosting (XGBoost, LightGBM, CatBoost)** | • Manejo de valores nulos (algunos modelos manejan nulos nativamente)<br>• Codificación de variables categóricas (CatBoost maneja categóricas nativamente)<br>• No requiere normalización<br>• Balanceo de clases para datos desbalanceados | • Tasa de aprendizaje<br>• Número de árboles<br>• Profundidad máxima<br>• Regularización (gamma, alpha, lambda)<br>• Submuestra<br>• Ratio de columnas por árbol<br>• min_child_weight<br>• early_stopping_rounds | • Precisión, Recall, F1-score<br>• Importancia de características<br>• Log loss<br>• AUC-ROC<br>• RMSE (para regresión) |
| **SVM (Support Vector Machines)** | • Normalización/Estandarización (crucial)<br>• Manejo de valores nulos<br>• Codificación de variables categóricas<br>• Reducción de dimensionalidad para datasets grandes | • Tipo de kernel (linear, rbf, poly)<br>• C (regularización)<br>• Gamma (para kernels rbf)<br>• Grado (para kernel polinomial)<br>• Coef0<br>• Tolerancia<br>• Probabilidad (para estimaciones de probabilidad) | • Precisión, Recall, F1-score<br>• AUC-ROC<br>• Support vectors ratio<br>• Margen de separación |

## 2. Modelos para Procesamiento de Lenguaje Natural (NLP)

| Modelo | Preprocesamiento de Datos | Hiperparámetros Clave | Métricas de Evaluación |
|--------|---------------------------|------------------------|------------------------|
| **Bag of Words (BoW) / TF-IDF** | • Tokenización<br>• Eliminación de stopwords<br>• Lematización/Stemming<br>• Normalización de texto<br>• Eliminación de caracteres especiales<br>• Corrección ortográfica | • Tamaño del vocabulario<br>• N-gramas (rango)<br>• Frecuencia mínima de documentos<br>• Frecuencia máxima de documentos<br>• Normalización (l1, l2) | • Se evalúa con el modelo que lo utiliza<br>• Cobertura de vocabulario<br>• Sparsity |
| **Word2Vec / GloVe / FastText** | • Tokenización<br>• Eliminación de stopwords (opcional)<br>• Normalización de texto<br>• Lematización/Stemming (opcional) | • Dimensiones del embedding<br>• Tamaño de ventana<br>• Arquitectura (CBOW o Skip-gram para Word2Vec)<br>• Muestreo negativo<br>• Mínima frecuencia de palabras<br>• Epochs | • Analogías semánticas<br>• Similitud coseno<br>• Tareas downstream<br>• Cobertura OOV (para FastText) |
| **LSTM / GRU** | • Tokenización<br>• Padding/Truncación de secuencias<br>• Embedding (pre-entrenado o no)<br>• Normalización de texto | • Unidades ocultas<br>• Número de capas<br>• Dropout<br>• Bidireccionalidad<br>• Tamaño de batch<br>• Tasa de aprendizaje<br>• Función de activación<br>• Recurrent dropout | • Perplexidad<br>• BLEU (generación)<br>• Precisión, Recall, F1 (clasificación)<br>• Cross-entropy |
| **Transformers (BERT, GPT, T5)** | • Tokenización especial (WordPiece, BPE, etc.)<br>• Truncación/Padding de secuencias<br>• Manejo de atención enmascarada<br>• Estructura de entrada especial ([CLS], [SEP], etc.) | • Tasa de aprendizaje<br>• Tamaño de batch<br>• Número de epochs<br>• Warmup steps<br>• Weight decay<br>• Sequence length<br>• Fine-tuning layers<br>• Estrategia de optimización | • GLUE/SuperGLUE benchmarks<br>• Perplexidad<br>• BLEU, ROUGE, METEOR<br>• Exactitud, F1-score<br>• Matriz de confusión |

## 3. Modelos para Visión por Computadora

| Modelo | Preprocesamiento de Datos | Hiperparámetros Clave | Métricas de Evaluación |
|--------|---------------------------|------------------------|------------------------|
| **CNN (ResNet, VGG, EfficientNet)** | • Redimensionamiento<br>• Normalización (media/std)<br>• Aumento de datos (rotación, zoom, flip)<br>• Centralización<br>• Conversión de canal (RGB, grayscale)<br>• Ecualización de histograma | • Arquitectura<br>• Tasa de aprendizaje<br>• Tamaño de batch<br>• Optimizador<br>• Dropout<br>• Batch normalization<br>• Weight decay<br>• Programación de tasa de aprendizaje | • Precisión (Top-1, Top-5)<br>• F1-score<br>• IoU (para segmentación)<br>• Recall@k<br>• Matriz de confusión<br>• AUC-ROC |
| **YOLO, SSD, Faster R-CNN (Detección de Objetos)** | • Redimensionamiento<br>• Aumento de datos específico para detección<br>• Normalización<br>• Generación de anclas/cajas<br>• Balanceo de clases | • Arquitectura base<br>• IoU threshold<br>• Confidence threshold<br>• NMS threshold<br>• Anchor sizes/ratios<br>• Tasa de aprendizaje<br>• Regularización<br>• Balanceo de pérdidas | • mAP (mean Average Precision)<br>• IoU<br>• Recall<br>• Precisión<br>• F1-score<br>• FPS (frames por segundo) |
| **U-Net, DeepLabv3+ (Segmentación)** | • Redimensionamiento<br>• Normalización<br>• Aumento de datos específico para segmentación<br>• Codificación de máscaras<br>• Balanceo de clases | • Arquitectura backbone<br>• Funciones de pérdida (Dice, CE, Focal)<br>• Decoder channels<br>• Tasa de aprendizaje<br>• Optimizador<br>• Batch size<br>• Weight decay | • IoU (Intersection over Union)<br>• Dice coefficient<br>• Pixel accuracy<br>• F1-score<br>• Precisión, Recall<br>• Mean BFscore |
| **GANs (Generación)** | • Redimensionamiento<br>• Normalización (-1 a 1)<br>• Aumento de datos<br>• Filtrado de datos<br>• Preprocesamiento específico del dominio | • Arquitectura G/D<br>• Tasa de aprendizaje G/D<br>• Ratio de entrenamiento G/D<br>• Ruido latente<br>• Batch size<br>• Momentum<br>• Regularización<br>• Función de pérdida | • FID (Fréchet Inception Distance)<br>• Inception Score<br>• SSIM<br>• PSNR<br>• Precisión/Recall<br>• MMD |

## 4. Modelos para Series Temporales

| Modelo | Preprocesamiento de Datos | Hiperparámetros Clave | Métricas de Evaluación |
|--------|---------------------------|------------------------|------------------------|
| **ARIMA / SARIMA** | • Verificación de estacionariedad (test KPSS, ADF)<br>• Diferenciación<br>• Transformación (Box-Cox, log)<br>• Manejo de valores nulos<br>• Detección de outliers<br>• Descomposición (tendencia, estacionalidad) | • p (orden AR)<br>• d (orden de diferenciación)<br>• q (orden MA)<br>• P, D, Q (parámetros estacionales)<br>• s (período estacional)<br>• Método de estimación | • RMSE<br>• MAE<br>• MAPE<br>• AIC/BIC<br>• Ljung-Box test<br>• Análisis de residuos |
| **Prophet** | • Manejo de outliers<br>• Transformación de datos<br>• Ajuste de fechas<br>• Incorporación de festivos/eventos<br>• Manejo de valores nulos | • Punto de cambio (changepoint_prior_scale)<br>• Estacionalidad (seasonality_prior_scale)<br>• Capacidad de crecimiento<br>• Holidays_prior_scale<br>• Número de puntos de cambio<br>• Estacionalidad (diaria, semanal, anual) | • RMSE<br>• MAE<br>• MAPE<br>• Crossvalidation<br>• Cobertura de intervalos de predicción |
| **LSTM / GRU para Series** | • Normalización/Estandarización<br>• Creación de secuencias (sliding window)<br>• Transformación estacionaria<br>• Codificación de características temporales<br>• Manejo de valores nulos | • Unidades de memoria<br>• Capas<br>• Window size<br>• Horizon size<br>• Dropout<br>• Tasa de aprendizaje<br>• Batch size<br>• Función de activación<br>• Recurrent dropout | • RMSE<br>• MAE<br>• MAPE<br>• SMAPE<br>• Coeficiente de determinación (R²) |

## 5. Modelos de Recomendación

| Modelo | Preprocesamiento de Datos | Hiperparámetros Clave | Métricas de Evaluación |
|--------|---------------------------|------------------------|------------------------|
| **Filtrado Colaborativo** | • Normalización de ratings<br>• Eliminación de usuarios/ítems con pocas interacciones<br>• Transformación de interacciones implícitas<br>• Muestreo negativo | • Número de factores latentes<br>• Regularización<br>• Algoritmo de optimización<br>• Learning rate<br>• Inicialización<br>• Bias incorporation | • RMSE<br>• MAE<br>• Precisión@k<br>• Recall@k<br>• MAP@k<br>• NDCG@k |
| **Factorización de Matrices** | • Normalización de ratings<br>• Manejo de valores faltantes<br>• Muestreo negativo<br>• Transformación de matrices dispersas | • Número de factores<br>• Regularización<br>• Learning rate<br>• Epochs<br>• Confidence weighting<br>• Bias terms | • RMSE<br>• MAE<br>• Precisión@k<br>• Recall@k<br>• AUC<br>• Cobertura |
| **Recomendación basada en Contenido** | • Extracción de características<br>• Normalización de características<br>• TF-IDF para características textuales<br>• Codificación one-hot<br>• Reducción de dimensionalidad | • Función de similitud<br>• Umbral de similitud<br>• Ponderación de características<br>• Número de vecinos (k)<br>• Algoritmo de clustering | • Precisión@k<br>• Recall@k<br>• F1-score<br>• Diversidad<br>• Novedad<br>• Cobertura |
